# PhD-project

For more details on raw data & HPC usage, please refer to [this documentation](https://docs.google.com/document/d/1pJ6w0NcR076oyPCq2k2JkogblUKxICGdEs_JHLW4bsA/edit?usp=sharing).

## Data 
The following files are needed to run the scripts. 

- `larn_data_smaller.pt`



## Command Line Arguments 

```
python ARM.py
```

## Files and Directories 
- `ARM/` 
  - `Topic/` 
    - `info*` -  outputs from [BERTopic](https://maartengr.github.io/BERTopic/api/bertopic.html). There are 10 clusters of topics per pair of country, each cluster has 10 keywords. This file contains the [c-TF-IDF score](https://maartengr.github.io/BERTopic/api/ctfidf.html) of each keywords. 
    - `prob*` - outputs from BERTopic. They are the probability of the assigned topic per document.
    - `embeddings*` -  topic embeddings by multiplying document embeddings (generated by [sentence transformer](https://www.sbert.net/examples/applications/computing-embeddings/README.html) with pretrained model *all-mpnet-base-v2*) with `prob*`.  
  - `sbatch/` - the sbatch files that can be submitted to HPC. Please refer to the documentation if you need more info of HPC. 
  - `TopicEmbedding.ipynb` - the script used to create `info*`, `prob*` and `embeddings*`. 
  - `ARM.py` - the main script to create 
  - `TemporalTrend.py`
  - `constants.py`
  - `modules.py`
  - `preprocessing.py`
  - `utils.py`
- `LARN-all-data/` contains the LARN model which can be run with all data (e.g., US-RU news data from 2016-2019). It does not have the topic components. 
- `LARN/`
