# PhD-project

For more details on NOW corpus & HPC usage, please refer to [this documentation](https://docs.google.com/document/d/1pJ6w0NcR076oyPCq2k2JkogblUKxICGdEs_JHLW4bsA/edit?usp=sharing).

## Data 
The following files are needed in the scripts. They can be accessd via [`/RelMod/model_input/`](https://webdata.tudelft.nl/staff-umbrella/RelMod/model_input/) in TU Delft Webdata, unless otherwise stated. 

The following three files have to be placed in the parent folder of `ARM/`:
- `large_model_input_gs1.pkl` - 21,98 GB. It can be downloaded [here](https://drive.google.com/file/d/1NVzG8RwEKiyk-dMIFUOicELLEQhMAFh8/view). This link is from LARN paper. 
- `larn_data_smaller.pt` - 11,45 GB. The same contents as `large_model_input_gs.pkl` but the data type is 32-bit. You can chhose to use this one or the previous one (which might depend on how much memory you have). 
- `target_word_ix_counter.pk` - 234KB. Counts of each unique word in the corpus. 

The following two files are included in this github repository:
- `embeddings*` - topic embeddings. They are included in this repository under the folder `/ARM/Topic/`.   
- `trained-model*` - outputs from ARM model.  


## Command Line Arguments 

To train the model, you can run the following command line. To run the model on HPC, please use the sbatch files in the folder `ARM/sbatch/`.
```
python ARM.py
```

To produce the temporal trend plot, you can run the following command line. 
```
python TemporalTrend.py
```



## Files and Directories 
- `ARM/` 
  - `Topic/` 
    - `info*` - outputs from [BERTopic](https://maartengr.github.io/BERTopic/api/bertopic.html). There are 10 clusters of topics per pair of country, each cluster has 10 keywords. This file contains the [c-TF-IDF score](https://maartengr.github.io/BERTopic/api/ctfidf.html) of each keywords. 
    - `prob*` - outputs from BERTopic. They are the probability of the assigned topic per document.
    - `embeddings*` -  topic embeddings by multiplying document embeddings (generated by [sentence transformer](https://www.sbert.net/examples/applications/computing-embeddings/README.html) with pretrained model *all-mpnet-base-v2*) with `prob*`.  
  - `sbatch/` - the sbatch files that can be submitted to HPC. Please refer to the documentation if you need more info of HPC. 
  - `TopicEmbedding.ipynb` - the script used to create `info*`, `prob*` and `embeddings*`. 
  - `ARM.py` - trains the model.    
  - `TemporalTrend.py` - produces temporal trend per topic. The output plots are in `plot/` in this github repository.
  - `constants.py` - 
  - `modules.py` - 
  - `preprocessing.py`
  - `utils.py`
- `LARN-all-data/` contains the LARN model which can be run with all data (e.g., US-RU news data from 2016-2019). It does not have the topic components. 
- `LARN/`
